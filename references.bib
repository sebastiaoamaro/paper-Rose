@inproceedings {AlagappanEtAl16-OSDI,
author = {Ramnatthan Alagappan and Aishwarya Ganesan and Yuvraj Patel and Thanumalayan Sankaranarayana Pillai and Andrea C. Arpaci-Dusseau and Remzi H. Arpaci-Dusseau},
title = {Correlated Crash Vulnerabilities},
booktitle = {12th USENIX Symposium on Operating Systems Design and Implementation (OSDI 16)},
year = {2016},
isbn = {978-1-931971-33-1},
address = {GA},
pages = {151--167},
url = {https://www.usenix.org/conference/osdi16/technical-sessions/presentation/alagappan},
publisher = {USENIX Association},
}

@INPROCEEDINGS{chronos,
  author={Chen, Yuanliang and Ma, Fuchen and Zhou, Yuanhang and Gu, Ming and Liao, Qing and Jiang, Yu},
  booktitle={2024 IEEE Symposium on Security and Privacy (SP)},
  title={Chronos: Finding Timeout Bugs in Practical Distributed Systems by Deep-Priority Fuzzing with Transient Delay},
  year={2024},
  volume={},
  number={},
  pages={1939-1955},
  keywords={Privacy;Runtime;Computer bugs;Life estimation;Fuzzing;Libraries;Delays},
  doi={10.1109/SP54263.2024.00109}}

@misc{usdt,
	Title = {User Statically-Defined Tracing.},
	url = {https://docs.ebpf.io/linux/concepts/usdt/},
	note = {Accessed: 2025-09-10}
}

@misc{aspectj,
	Title = {AspectJ.},
	url = {https://eclipse.dev/aspectj/},
	note = {Accessed: 2025-09-10}
}

@misc{redisraft2d1cf30,
	Title = {RedisRaft commit 2d1cf30.},
	url = {https://github.com/RedisLabs/redisraft/commit/2d1cf30},
	note = {Accessed: 2025-05-15}
}

@misc{redisraftd1d728d,
	Title = {RedisRaft commit d1d728d.},
	url = {https://github.com/RedisLabs/redisraft/commit/d1d728d},
	note = {Accessed: 2025-05-15}
}

@misc{ebpf,
	Title = {eBPF.},
	url = {https://ebpf.io/},
	note = {Accessed: 2025-05-15}
}

@misc{zookeeper2247,
	Title = {Zookeeper issue 2207.},
	url = {https://issues.apache.org/jira/browse/ZOOKEEPER-2247},
	note = {Accessed: 2025-05-15}
}

@misc{zookeeper3006,
	Title = {Zookeeper issue 3006.},
	url = {https://issues.apache.org/jira/browse/ZOOKEEPER-3006},
	note = {Accessed: 2025-05-15}
}

@misc{zookeeper3157,
	Title = {Zookeeper issue 3157.},
	url = {https://issues.apache.org/jira/browse/ZOOKEEPER-3157},
	note = {Accessed: 2025-05-15}
}

@misc{zookeeper4203,
	Title = {Zookeeper issue 4203.},
	url = {https://issues.apache.org/jira/browse/ZOOKEEPER-4203},
	note = {Accessed: 2025-05-15}
}

@misc{hdfs4233,
	Title = {HDFS issue 4233.},
	url = {https://issues.apache.org/jira/browse/hdfs-4233},
	note = {Accessed: 2025-05-15}
}

@misc{hdfs12070,
	Title = {HDFS issue 12070.},
	url = {https://issues.apache.org/jira/browse/hdfs-12070},
	note = {Accessed: 2025-05-15}
}

@misc{hdfs15032,
	Title = {HDFS issue 15032.},
	url = {https://issues.apache.org/jira/browse/hdfs-15032},
	note = {Accessed: 2025-05-15}
}

@misc{hdfs16332,
	Title = {HDFS issue 16332.},
	url = {https://issues.apache.org/jira/browse/hdfs-16332},
	note = {Accessed: 2025-05-15}
}

@misc{kafka12508,
	Title = {Kafka issue 12508.},
	url = {https://issues.apache.org/jira/browse/hdfs-12508},
	note = {Accessed: 2025-05-15}
}

@misc{hbase19608,
	Title = {HBASE issue 19608.},
	url = {https://issues.apache.org/jira/browse/hdfs-19608},
	note = {Accessed: 2025-05-15}
}

@misc{tendermint5839,
	Title = {Tendermint issue 5839.},
	url = {https://github.com/tendermint/tendermint/issues/5839},
	note = {Accessed: 2025-05-15}
}

@misc{redpanda3003,
	Title = {Redpanda issue 3003.},
	url = {https://github.com/redpanda-data/redpanda/pull/3003},
	note = {Accessed: 2025-05-15}
}

@misc{redpanda3039,
	Title = {Redpanda issue 3039.},
	url = {https://github.com/redpanda-data/redpanda/pull/3039},
	note = {Accessed: 2025-05-15}
}

@misc{redisraft42,
	Title = {RedisRaft issue 42.},
	url = {https://github.com/RedisLabs/redisraft/issues/42},
	note = {Accessed: 2025-05-15}}
@misc{redisraft51,
	Title = {RedisRaft issue 51.},
	url = {https://github.com/RedisLabs/redisraft/issues/51},
	note = {Accessed: 2025-05-15}}

@misc{redisraftnew,
	Title = {RedisRaft issue 654.},
	url = {https://github.com/RedisLabs/redisraft/issues/654},
	note = {Accessed: 2025-05-15}}


@misc{redisraftreport,
	Title = {Jepsen analysis of RedisRaft.},
	url = {https://jepsen.io/analyses/redis-raft-1b3fbf6},
	note = {Accessed: 2025-05-15}}

@misc{redisraft,
	Title = {RedisRaft repository.},
	url = {https://github.com/RedisLabs/redisraft},
	note = {Accessed: 2025-05-15}}


@misc{redpanda,
	Title = {Redpanda, a Kafka® compatible streaming data platform.},
	url = {https://www.redpanda.com/},
	note = {Accessed: 2025-1-01}}


@misc{redpandareport,
	Title = {Jepsen analysis of Redpanda.},
	url = {https://jepsen.io/analyses/redpanda-21.10.1},
	note = {Accessed: 2025-05-05}}


@misc{openaioutage,
    Title = {Incident Report: High error rates for ChatGPT, APIs, and Sora, 2025},
    url ={https://status.openai.com/incidents/6bwlxnvdncnm},
    note = {Accessed: 2025-5-13}}

@misc{spotifyoutage,
    Title = {Incident Report: Spotify Outage on April 16, 2025},
    url = {https://engineering.atspotify.com/2025/05/incident-report-spotify-outage-on-april-16-2025},
    note = {Accessed: 2025-5-13}}

@misc{googleoutage,
    Title = {Google lost \$1.7M in ad revenue during YouTube outage},
    url = {https://www.foxbusiness.com/technology/google-lost-ad-revenue-during-youtube-outage-expert},
    note = {Accessed: 2025-1-01}}

@misc{mongo-3-2-10,
    Title = {MongoDB Unavailability bug},
    url = {https://jira.mongodb.org/browse/SERVER-27125},
    note = {Accessed: 2025-05-14}}

@misc{mongo-2-4-3,
    Title = {Jepsen MongoDB:2.4.3},
    url = {https://aphyr.com/posts/284-call-me-maybe-mongodb},
    note = {Accessed: 2025-05-14}}
@misc{heatwaves,
    Title = {Google’s London data center outage during heatwave},
    url = {https://www.reuters.com/technology/google-cloud-data-center-london-faces-outage-uks-hottest-day-2022-07-19/},
    note = {Accessed: 2025-1-01}}


@misc{lightstrikes,
    Title = {Lightning in Belgium Disrupts Google Cloud Services},
    url = {https://www.datacenterknowledge.com/archives/2015/08/19/lightning-strikes-google-data-center-disrupts-cloud-services},
    note = {Accessed: 2025-1-01}}


@misc{bugsIT,
    Title = {Google Dashoard Bug Incident},
    url = {https://status.cloud.google.com/incident/zall/20013},
    note = {Accessed:2025-1-01}}

@misc{humanerror,
    Title = {Data Center Backup Failure Blamed on 1 Person},
    url = {https://www.datacenterknowledge.com/manage/data-center-backup-failure-blamed-1-person-not-nyse-leadership},
    note = {Accessed: 2025-1-01}}


@misc{libbpf,
	Title = {Documentation for libbpf, a userspace library for loading and interacting with bpf programs.},
	url = {https://libbpf.readthedocs.io/en/latest/index.html},
	note = {Accessed: 2025-1-01}}

@misc{bcc,
	Title = {BCC github repository},
	url = {https://github.com/iovisor/bcc},
	note = {Accessed: 2025-1-01}}


@misc{cgroups,
	Title = {cgroups(7) — Linux manual page},
	url = { https://man7.org/linux/man-pages/man7/cgroups.7.html},
	note = {Accessed: 2025-1-01}}

@misc{chaosmonkey,
	Title = {Chaos monkey github},
	url = { https://github.com/netflix/chaosmonkey},
	note = {Accessed: 2025-1-01}}

 @inproceedings {partial-network-failures,
author = {Mohammed Alfatafta and Basil Alkhatib and Ahmed Alquraan and Samer Al-Kiswany},
title = {Toward a Generic Fault Tolerance Technique for Partial Network Partitioning},
booktitle = {14th USENIX Symposium on Operating Systems Design and Implementation (OSDI 20)},
year = {2020},
isbn = {978-1-939133-19-9},
pages = {351--368},
url = {https://www.usenix.org/conference/osdi20/presentation/alfatafta},
publisher = {USENIX Association},
month = nov
}

@inproceedings{fcatch,
author = {Liu, Haopeng and Wang, Xu and Li, Guangpu and Lu, Shan and Ye, Feng and Tian, Chen},
title = {FCatch: Automatically Detecting Time-of-Fault Bugs in Cloud Systems},
year = {2018},
isbn = {9781450349116},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3173162.3177161},
doi = {10.1145/3173162.3177161},
abstract = {It is crucial for distributed systems to achieve high availability. Unfortunately, this is challenging given the common component failures (i.e., faults). Developers often cannot anticipate all the timing conditions and system states under which a fault might occur, and introduce time-of-fault (TOF) bugs that only manifest when a node crashes or a message drops at a special moment. Although challenging, detecting TOF bugs is fundamental to developing highly available distributed systems. Unlike previous work that relies on fault injection to expose TOF bugs, this paper carefully models TOF bugs as a new type of concurrency bugs, and develops FCatch to automatically predict TOF bugs by observing correct execution. Evaluation on representative cloud systems shows that FCatch is effective, accurately finding severe TOF bugs.},
booktitle = {Proceedings of the Twenty-Third International Conference on Architectural Support for Programming Languages and Operating Systems},
pages = {419–431},
numpages = {13},
keywords = {timing bugs, distributed systems, fault tolerance, cloud computing, bug detection},
location = {Williamsburg, VA, USA},
series = {ASPLOS '18}
}

@inproceedings{crashmonkey18mohan,
  title= "{Finding Crash-Consistency Bugs with Bounded Black-Box Crash Testing}",
  author = {Mohan, Jayashree and Martinez, Ashlie and Ponnapalli,
                  Soujanya and Raju, Pandian and Chidambaram, Vijay},
  booktitle= "{13th USENIX Symposium on Operating Systems Design and Implementation (OSDI 2018)}",
  organization={USENIX Association},
  address = {Carlsbad, CA},
  year={2018}
}

@INPROCEEDINGS{9237004,
  author={Amaral, Miguel and Pardal, Miguel L. and Mercier, Hugues and Matos, Miguel},
  booktitle={2020 16th European Dependable Computing Conference (EDCC)},
  title={FaultSee: Reproducible Fault Injection in Distributed Systems},
  year={2020},
  volume={},
  number={},
  pages={25-32},
  doi={10.1109/EDCC51268.2020.00014}}


@inproceedings{10.1145/2723372.2723711,
author = {Alvaro, Peter and Rosen, Joshua and Hellerstein, Joseph M.},
title = {Lineage-Driven Fault Injection},
year = {2015},
isbn = {9781450327589},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2723372.2723711},
doi = {10.1145/2723372.2723711},
abstract = {In large-scale data management systems, failure is practically a certainty. Fault-tolerant protocols and components are notoriously difficult to implement and debug. Worse still, choosing existing fault-tolerance mechanisms and integrating them correctly into complex systems remains an art form, and programmers have few tools to assist them.We propose a novel approach for discovering bugs in fault-tolerant data management systems: lineage-driven fault injection. A lineage-driven fault injector reasons backwards from correct system outcomes to determine whether failures in the execution could have prevented the outcome. We present MOLLY, a prototype of lineage-driven fault injection that exploits a novel combination of data lineage techniques from the database literature and state-of-the-art satisfiability testing. If fault-tolerance bugs exist for a particular configuration, MOLLY finds them rapidly, in many cases using a order of magnitude fewer executions than random fault injection. Otherwise, MOLLY certifies that the code is bug-free for that configuration.},
booktitle = {Proceedings of the 2015 ACM SIGMOD International Conference on Management of Data},
pages = {331–346},
numpages = {16},
keywords = {fault-tolerance, verification, provenance},
location = {Melbourne, Victoria, Australia},
series = {SIGMOD '15}
}


@inproceedings{Dawson1996ORCHESTRAAF,
  title={ORCHESTRA: A Fault Injection Environment for Distributed Systems},
  author={Scott Dawson and Farnam Jahanian},
  year={1996}
}

@ARTICLE{364536,
  author={Kanawati, G.A. and Kanawati, N.A. and Abraham, J.A.},
  journal={IEEE Transactions on Computers},
  title={FERRARI: a flexible software-based fault and error injection system},
  year={1995},
  volume={44},
  number={2},
  pages={248-260},
  doi={10.1109/12.364536}}


@article{DBLP:journals/corr/BasiriBRHKRR17,
  author    = {Ali Basiri and
               Niosha Behnam and
               Ruud de Rooij and
               Lorin Hochstein and
               Luke Kosewski and
               Justin Reynolds and
               Casey Rosenthal},
  title     = {Chaos Engineering},
  journal   = {CoRR},
  volume    = {abs/1702.05843},
  year      = {2017},
  url       = {http://arxiv.org/abs/1702.05843},
  eprinttype = {arXiv},
  eprint    = {1702.05843},
  timestamp = {Mon, 13 Aug 2018 16:47:10 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/BasiriBRHKRR17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}



@inproceedings{10.1145/3037697.3037735,
author = {Liu, Haopeng and Li, Guangpu and Lukman, Jeffrey F. and Li, Jiaxin and Lu, Shan and Gunawi, Haryadi S. and Tian, Chen},
title = {DCatch: Automatically Detecting Distributed Concurrency Bugs in Cloud Systems},
year = {2017},
isbn = {9781450344654},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3037697.3037735},
doi = {10.1145/3037697.3037735},
abstract = {In big data and cloud computing era, reliability of distributed systems is extremely important. Unfortunately, distributed concurrency bugs, referred to as DCbugs, widely exist. They hide in the large state space of distributed cloud systems and manifest non-deterministically depending on the timing of distributed computation and communication. Effective techniques to detect DCbugs are desired. This paper presents a pilot solution, DCatch, in the world of DCbug detection. DCatch predicts DCbugs by analyzing correct execution of distributed systems. To build DCatch, we design a set of happens-before rules that model a wide variety of communication and concurrency mechanisms in real-world distributed cloud systems. We then build runtime tracing and trace analysis tools to effectively identify concurrent conflicting memory accesses in these systems. Finally, we design tools to help prune false positives and trigger DCbugs. We have evaluated DCatch on four representative open-source distributed cloud systems, Cassandra, Hadoop MapReduce, HBase, and ZooKeeper. By monitoring correct execution of seven workloads on these systems, DCatch reports 32 DCbugs, with 20 of them being truly harmful.},
booktitle = {Proceedings of the Twenty-Second International Conference on Architectural Support for Programming Languages and Operating Systems},
pages = {677–691},
numpages = {15},
keywords = {distributed systems, bug detection, concurrency bugs, cloud computing},
location = {Xi'an, China},
series = {ASPLOS '17}
}


@inproceedings{10.5555/3291168.3291172,
author = {Mohan, Jayashree and Martinez, Ashlie and Ponnapalli, Soujanya and Raju, Pandian and Chidambaram, Vijay},
title = {Finding Crash-Consistency Bugs with Bounded Black-Box Crash Testing},
year = {2018},
isbn = {9781931971478},
publisher = {USENIX Association},
address = {USA},
abstract = {We present a new approach to testing file-system crash consistency: bounded black-box crash testing (B3). B3 tests the file system in a black-box manner using workloads of file-system operations. Since the space of possible workloads is infinite, B3 bounds this space based on parameters such as the number of file-system operations or which operations to include, and exhaustively generates workloads within this bounded space. Each workload is tested on the target file system by simulating power-loss crashes while the workload is being executed, and checking if the file system recovers to a correct state after each crash. B3 builds upon insights derived from our study of crash-consistency bugs reported in Linux file systems in the last five years. We observed that most reported bugs can be reproduced using small workloads of three or fewer file-system operations on a newly-created file system, and that all reported bugs result from crashes after fsync() related system calls. We build two tools, CRASHMONKEY and ACE, to demonstrate the effectiveness of this approach. Our tools are able to find 24 out of the 26 crash-consistency bugs reported in the last five years. Our tools also revealed 10 new crash-consistency bugs in widely-used, mature Linux file systems, seven of which existed in the kernel since 2014. The new bugs result in severe consequences like broken rename atomicity and loss of persisted files.},
booktitle = {Proceedings of the 13th USENIX Conference on Operating Systems Design and Implementation},
pages = {33–50},
numpages = {18},
location = {Carlsbad, CA, USA},
series = {OSDI'18}
}


@article{elle,
  author    = {Kyle Kingsbury and
               Peter Alvaro},
  title     = {Elle: Inferring Isolation Anomalies from Experimental Observations},
  journal   = {CoRR},
  volume    = {abs/2003.10554},
  year      = {2020},
  url       = {https://arxiv.org/abs/2003.10554},
  eprinttype = {arXiv},
  eprint    = {2003.10554},
  timestamp = {Wed, 01 Apr 2020 17:39:12 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2003-10554.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@INPROCEEDINGS{9153386,
  author={Cotroneo, Domenico and De Simone, Luigi and Liguori, Pietro and Natella, Roberto},
  booktitle={2020 50th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN)},
  title={ProFIPy: Programmable Software Fault Injection as-a-Service},
  year={2020},
  volume={},
  number={},
  pages={364-372},
  doi={10.1109/DSN48063.2020.00052}}


@inproceedings {sieve,
author = {Xudong Sun and Wenqing Luo and Jiawei Tyler Gu and Aishwarya Ganesan and Ramnatthan Alagappan and Michael Gasch and Lalith Suresh and Tianyin Xu},
title = {Automatic Reliability Testing For Cluster Management Controllers},
booktitle = {16th USENIX Symposium on Operating Systems Design and Implementation (OSDI 22)},
year = {2022},
isbn = {978-1-939133-28-1},
address = {Carlsbad, CA},
pages = {143--159},
url = {https://www.usenix.org/conference/osdi22/presentation/sun},
publisher = {USENIX Association},
month = jul,
}

@inproceedings{10.1145/3338906.3338961,
author = {Zhou, Xiang and Peng, Xin and Xie, Tao and Sun, Jun and Ji, Chao and Liu, Dewei and Xiang, Qilin and He, Chuan},
title = {Latent Error Prediction and Fault Localization for Microservice Applications by Learning from System Trace Logs},
year = {2019},
isbn = {9781450355728},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3338906.3338961},
doi = {10.1145/3338906.3338961},
abstract = {In the production environment, a large part of microservice failures are related to the complex and dynamic interactions and runtime environments, such as those related to multiple instances, environmental configurations, and asynchronous interactions of microservices. Due to the complexity and dynamism of these failures, it is often hard to reproduce and diagnose them in testing environments. It is desirable yet still challenging that these failures can be detected and the faults can be located at runtime of the production environment to allow developers to resolve them efficiently. To address this challenge, in this paper, we propose MEPFL, an approach of latent error prediction and fault localization for microservice applications by learning from system trace logs. Based on a set of features defined on the system trace logs, MEPFL trains prediction models at both the trace level and the microservice level using the system trace logs collected from automatic executions of the target application and its faulty versions produced by fault injection. The prediction models thus can be used in the production environment to predict latent errors, faulty microservices, and fault types for trace instances captured at runtime. We implement MEPFL based on the infrastructure systems of container orchestrator and service mesh, and conduct a series of experimental studies with two opensource microservice applications (one of them being the largest open-source microservice application to our best knowledge). The results indicate that MEPFL can achieve high accuracy in intraapplication prediction of latent errors, faulty microservices, and fault types, and outperforms a state-of-the-art approach of failure diagnosis for distributed systems. The results also show that MEPFL can effectively predict latent errors caused by real-world fault cases.},
booktitle = {Proceedings of the 2019 27th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {683–694},
numpages = {12},
keywords = {debugging, tracing, microservices, machine learning, fault localization, error prediction},
location = {Tallinn, Estonia},
series = {ESEC/FSE 2019}
}

@inproceedings{10.1145/3341301.3359645,
author = {Lu, Jie and Liu, Chen and Li, Lian and Feng, Xiaobing and Tan, Feng and Yang, Jun and You, Liang},
title = {CrashTuner: Detecting Crash-Recovery Bugs in Cloud Systems via Meta-Info Analysis},
year = {2019},
isbn = {9781450368735},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3341301.3359645},
doi = {10.1145/3341301.3359645},
abstract = {Crash-recovery bugs (bugs in crash-recovery-related mechanisms) are among the most severe bugs in cloud systems and can easily cause system failures. It is notoriously difficult to detect crash-recovery bugs since these bugs can only be exposed when nodes crash under special timing conditions. This paper presents CrashTuner, a novel fault-injection testing approach to combat crash-recovery bugs. The novelty of CrashTuner lies in how we identify fault-injection points (crash points) that are likely to expose errors. We observe that if a node crashes while accessing meta-info variables, i.e., variables referencing high-level system state information (e.g., an instance of node or task), it often triggers crash-recovery bugs. Hence, we identify crash points by automatically inferring meta-info variables via a log-based static program analysis. Our approach is automatic and no manual specification is required.We have applied CrashTuner to five representative distributed systems: Hadoop2/Yarn, HBase, HDFS, ZooKeeper, and Cassandra. CrashTuner can finish testing each system in 17.39 hours, and reports 21 new bugs that have never been found before. All new bugs are confirmed by the original developers and 16 of them have already been fixed (14 with our patches). These new bugs can cause severe damages such as cluster down or start-up failures.},
booktitle = {Proceedings of the 27th ACM Symposium on Operating Systems Principles},
pages = {114–130},
numpages = {17},
keywords = {crash recovery bugs, cloud computing, fault injection, distributed systems, fault tolerance, bug detection},
location = {Huntsville, Ontario, Canada},
series = {SOSP '19}
}

https://github.com/torvalds/linux/blob/v6.7/arch/x86/entry/syscalls/syscall_64.tbl

@misc{syscallids,
	Title = {System call identifiers.},
	url = {	https://github.com/torvalds/linux/blob/v6.7/arch/x86/entry/syscalls/syscall_64.tbl},
	note = {Accessed: 2025-5-11}}
@misc{XDP,
	Title = {eXpress Data Path},
	url = {	https://docs.cilium.io/en/stable/bpf/},
	note = {Accessed: 2025-2-1}}

 @misc{override,
	Title = {bpf-helpers},
	url = {	https://man7.org/linux/man-pages/man7/bpf-helpers.7.html},
	note = {Accessed: 2025-04-29}}

 @misc{send_signal,
	Title = {bpf-helpers},
	url = {	https://docs.ebpf.io/linux/helper-function/bpf_send_signal/},
	note = {Accessed: 2025-04-29}}



 @misc{syscalls,
	Title = {System Calls},
	url = {	https://man7.org/linux/man-pages/man2/syscalls.2.html},
	note = {Accessed: 2023-2-1}}

@misc{socketfilter,
	Title = {Linux socket filter},
	url = {	https://www.kernel.org/doc/Documentation/networking/filter.txt},
	note = {Accessed: 2023-2-1}}



  @misc{jepsen,
	Title = {Jepsen},
	url = {	https://github.com/jepsen-io/jepsen},
	note = {Accessed: 2023-2-1}}

@article{10.1145/3477133.3477135,
author = {Neves, Francisco and Vila\c{c}a, Ricardo and Pereira, Jos\'{e}},
title = {Detailed Black-Box Monitoring of Distributed Systems},
year = {2021},
issue_date = {March 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {1},
issn = {1559-6915},
url = {https://doi.org/10.1145/3477133.3477135},
doi = {10.1145/3477133.3477135},
abstract = {Modern containerized distributed systems, such as big data storage and processing stacks or micro-service based applications, are inherently hard to monitor and optimize, as resource usage does not directly match hardware resources due to multiple virtualization layers. For instance, interapplication traffic is an important factor in as it directly indicates how components interact, it has not been possible to accurately monitor it in an application independent way and without severe overhead, thus putting it out of reach of cloud platforms.In this paper we present an efficient black-box monitoring approach for gathering detailed structural information of collaborating processes in a distributed system that can be queried for various purposes, as it includes both information about processes, containers, and hosts, as well as resource usage and amount of data exchanged. The key to achieving high detail and low overhead without custom application instrumentation is to use a kernel-aided event driven strategy. We validate a prototype implementation by applying it to multi-platform microservice deployments, evaluate its performance with micro-benchmarks, and demonstrate its usefulness for container placement in a distributed data storage and processing stack (i.e., Cassandra and Spark).},
journal = {SIGAPP Appl. Comput. Rev.},
month = {jul},
pages = {24–36},
numpages = {13},
keywords = {containers, adaptive placement, monitoring}
}


@article{10.1145/2841425,
author = {Natella, Roberto and Cotroneo, Domenico and Madeira, Henrique S.},
title = {Assessing Dependability with Software Fault Injection: A Survey},
year = {2016},
issue_date = {February 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {48},
number = {3},
issn = {0360-0300},
url = {https://doi.org/10.1145/2841425},
doi = {10.1145/2841425},
abstract = {With the rise of software complexity, software-related accidents represent a significant threat for computer-based systems. Software Fault Injection is a method to anticipate worst-case scenarios caused by faulty software through the deliberate injection of software faults. This survey provides a comprehensive overview of the state of the art on Software Fault Injection to support researchers and practitioners in the selection of the approach that best fits their dependability assessment goals, and it discusses how these approaches have evolved to achieve fault representativeness, efficiency, and usability. The survey includes a description of relevant applications of Software Fault Injection in the context of fault-tolerant systems.},
journal = {ACM Comput. Surv.},
month = {feb},
articleno = {44},
numpages = {55},
keywords = {dependability assessment, Software faults, software fault tolerance}
}



 @inproceedings{kollaps,
author = {Gouveia, Paulo and Neves, Jo\~{a}o and Segarra, Carlos and Liechti, Luca and Issa, Shady and Schiavoni, Valerio and Matos, Miguel},
title = {Kollaps: Decentralized and Dynamic Topology Emulation},
year = {2020},
isbn = {9781450368827},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3342195.3387540},
doi = {10.1145/3342195.3387540},
booktitle = {Proceedings of the Fifteenth European Conference on Computer Systems},
articleno = {23},
numpages = {16},
keywords = {dynamic network topology, distributed systems, containers, experimental reproducibility, emulation},
location = {Heraklion, Greece},
series = {EuroSys '20}
}

@misc{perf,
	Title = { Perf Wiki.},
	url = {https://perf.wiki.kernel.org/index.php/Main\_Page},
	note = {Accessed: 2023-02-06}
}

@inproceedings{incidents1,
author = {Ghosh, Supriyo and Shetty, Manish and Bansal, Chetan and Nath, Suman},
title = {How to fight production incidents? an empirical study on a large-scale cloud service},
year = {2022},
isbn = {9781450394147},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3542929.3563482},
doi = {10.1145/3542929.3563482},
abstract = {Production incidents in today's large-scale cloud services can be extremely expensive in terms of customer impacts and engineering resources required to mitigate them. Despite continuous reliability efforts, cloud services still experience severe incidents due to various root-causes. Worse, many of these incidents last for a long period as existing techniques and practices fail to quickly detect and mitigate them. To better understand the problems, we carefully study hundreds of recent high severity incidents and their postmortems in Microsoft-Teams, a large-scale distributed cloud based service used by hundreds of millions of users. We answer: (a) why the incidents occurred and how they were resolved, (b) what the gaps were in current processes which caused delayed response, and (c) what automation could help make the services resilient. Finally, we uncover interesting insights by a novel multi-dimensional analysis that correlates different troubleshooting stages (detection, root-causing and mitigation), and provide guidance on how to tackle complex incidents through automation or testing at different granularity.},
booktitle = {Proceedings of the 13th Symposium on Cloud Computing},
pages = {126–141},
numpages = {16},
keywords = {distributed systems, empirical study, incident management, reliability},
location = {San Francisco, California},
series = {SoCC '22}
}

@inproceedings{incidents2,
author = {Liu, Haopeng and Lu, Shan and Musuvathi, Madan and Nath, Suman},
title = {What bugs cause production cloud incidents?},
year = {2019},
isbn = {9781450367271},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3317550.3321438},
doi = {10.1145/3317550.3321438},
abstract = {Cloud services have become the backbone of today's computing world. Runtime incidents, which adversely affect the expected service operations, are extremely costly in terms of user impacts and engineering efforts required to resolve them. Hence, such incidents are the target of much research effort. Unfortunately, there is limited understanding about cloud service incidents that actually happen during production runs: what cause them and how they are resolved.In this work, we carefully study hundreds of high-severity incidents that occurred recently during the production runs of many Microsoft Azure services. We find software bugs to be a major cause behind these incidents, and make interesting observations about the types of software bugs that cause cloud incidents and how these bug-related incidents are resolved, providing motivation and guidance to future research in tackling cloud bugs and improving the cloud-service availability.},
booktitle = {Proceedings of the Workshop on Hot Topics in Operating Systems},
pages = {155–162},
numpages = {8},
keywords = {Cloud system, bug characteristics, production incident, reliability},
location = {Bertinoro, Italy},
series = {HotOS '19}
}

@inproceedings{pensieve,
author = {Zhang, Yongle and Makarov, Serguei and Ren, Xiang and Lion, David and Yuan, Ding},
title = {Pensieve: Non-Intrusive Failure Reproduction for Distributed Systems using the Event Chaining Approach},
year = {2017},
isbn = {9781450350853},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132747.3132768},
doi = {10.1145/3132747.3132768},
abstract = {Complex and unforeseen failures in distributed systems must be diagnosed and replicated in a development environment so that developers can understand the underlying problem and verify the resolution. System logs often form the only source of diagnostic information, and developers reconstruct a failure using manual guesswork. This is an unpredictable and time-consuming process which can lead to costly service outages while a failure is repaired.This paper describes Pensieve, a tool capable of reconstructing near-minimal failure reproduction steps from log files and system bytecode, without human involvement. Unlike existing solutions that use symbolic execution to search for the entire path leading to the failure, Pensieve is based on the Partial Trace Observation, which states that programmers do not simulate the entire execution to understand the failure, but follow a combination of control and data dependencies to reconstruct a simplified trace that only contains events that are likely to be relevant to the failure. Pensieve follows a set of carefully designed rules to infer a chain of causally dependent events leading to the failure symptom while aggressively skipping unrelated code paths to avoid the path-explosion overheads of symbolic execution models.},
booktitle = {Proceedings of the 26th Symposium on Operating Systems Principles},
pages = {19–33},
numpages = {15},
keywords = {log, distributed systems, debugging, Failure reproduction},
location = {Shanghai, China},
series = {SOSP '17}
}

@inproceedings{r2,
author = {Guo, Zhenyu and Wang, Xi and Tang, Jian and Liu, Xuezheng and Xu, Zhilei and Wu, Ming and Kaashoek, M. Frans and Zhang, Zheng},
title = {R2: an application-level kernel for record and replay},
year = {2008},
publisher = {USENIX Association},
address = {USA},
abstract = {Library-based record and replay tools aim to reproduce an application's execution by recording the results of selected functions in a log and during replay returning the results from the log rather than executing the functions. These tools must ensure that a replay run is identical to the record run. The challenge in doing so is that only invocations of a function by the application should be recorded, recording the side effects of a function call can be difficult, and not executing function calls during replay, multithreading, and the presence of the tool may change the application's behavior from recording to replay. These problems have limited the use of such tools.R2 allows developers to choose functions that can be recorded and replayed correctly. Developers annotate the chosen functions with simple keywords so that R2 can handle calls with side effects andmultithreading. R2 generates code for record and replay from templates, allowing developers to avoid implementing stubs for hundreds of functions manually. To track whether an invocation is on behalf of the application or the implementation of a selected function, R2 maintains a mode bit, which stubs save and restore.We have implemented R2 on Windows and annotated large parts (1,300 functions) of the Win32 API, and two higher-level interfaces (MPI and SQLite). R2 can replay multithreaded web and database servers that previous library-based tools cannot replay. By allowing developers to choose high-level interfaces, R2 can also keep recording overhead small; experiments show that its recording overhead for Apache is approximately 10\%, that recording and replaying at the SQLite interface can reduce the log size up to 99\% (compared to doing so at the Win32 API), and that using optimization annotations for BitTorrent and MPI applications achieves log size reduction ranging from 13.7\% to 99.4\%.},
booktitle = {Proceedings of the 8th USENIX Conference on Operating Systems Design and Implementation},
pages = {193–208},
numpages = {16},
location = {San Diego, California},
series = {OSDI'08}
}


@inproceedings{respec,
author = {Lee, Dongyoon and Wester, Benjamin and Veeraraghavan, Kaushik and Narayanasamy, Satish and Chen, Peter M. and Flinn, Jason},
title = {Respec: efficient online multiprocessor replayvia speculation and external determinism},
year = {2010},
isbn = {9781605588391},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1736020.1736031},
doi = {10.1145/1736020.1736031},
abstract = {Deterministic replay systems record and reproduce the execution of a hardware or software system. While it is well known how to replay uniprocessor systems, replaying shared memory multiprocessor systems at low overhead on commodity hardware is still an open problem. This paper presents Respec, a new way to support deterministic replay of shared memory multithreaded programs on commodity multiprocessor hardware. Respec targets online replay in which the recorded and replayed processes execute concurrently.Respec uses two strategies to reduce overhead while still ensuring correctness: speculative logging and externally deterministic replay. Speculative logging optimistically logs less information about shared memory dependencies than is needed to guarantee deterministic replay, then recovers and retries if the replayed process diverges from the recorded process. Externally deterministic replay relaxes the degree to which the two executions must match by requiring only their system output and final program states match. We show that the combination of these two techniques results in low recording and replay overhead for the common case of data-race-free execution intervals and still ensures correct replay for execution intervals that have data races.We modified the Linux kernel to implement our techniques. Our software system adds on average about 18\% overhead to the execution time for recording and replaying programs with two threads and 55\% overhead for programs with four threads.},
booktitle = {Proceedings of the Fifteenth International Conference on Architectural Support for Programming Languages and Operating Systems},
pages = {77–90},
numpages = {14},
keywords = {external determinism, replay, speculative execution},
location = {Pittsburgh, Pennsylvania, USA},
series = {ASPLOS XV}
}

@inproceedings{ireplayer,
author = {Liu, Hongyu and Silvestro, Sam and Wang, Wei and Tian, Chen and Liu, Tongping},
title = {iReplayer: in-situ and identical record-and-replay for multithreaded applications},
year = {2018},
isbn = {9781450356985},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3192366.3192380},
doi = {10.1145/3192366.3192380},
abstract = {Reproducing executions of multithreaded programs is very challenging due to many intrinsic and external non-deterministic factors. Existing RnR systems achieve significant progress in terms of performance overhead, but none targets the in-situ setting, in which replay occurs within the same process as the recording process. Also, most existing work cannot achieve identical replay, which may prevent the reproduction of some errors.  This paper presents iReplayer, which aims to identically replay multithreaded programs in the original process (under the "in-situ" setting). The novel in-situ and identical replay of iReplayer makes it more likely to reproduce errors, and allows it to directly employ debugging mechanisms (e.g. watchpoints) to aid failure diagnosis. Currently, iReplayer only incurs 3\% performance overhead on average, which allows it to be always enabled in the production environment. iReplayer enables a range of possibilities, and this paper presents three examples: two automatic tools for detecting buffer overflows and use-after-free bugs, and one interactive debugging tool that is integrated with GDB.},
booktitle = {Proceedings of the 39th ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {344–358},
numpages = {15},
keywords = {Identical Replay, In-situ Replay, Multithreaded Debugging, Record-and-Replay},
location = {Philadelphia, PA, USA},
series = {PLDI 2018}
}
@inproceedings{doubleplay,
author = {Veeraraghavan, Kaushik and Lee, Dongyoon and Wester, Benjamin and Ouyang, Jessica and Chen, Peter M. and Flinn, Jason and Narayanasamy, Satish},
title = {DoublePlay: parallelizing sequential logging and replay},
year = {2011},
isbn = {9781450302661},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1950365.1950370},
doi = {10.1145/1950365.1950370},
abstract = {Deterministic replay systems record and reproduce the execution of a hardware or software system. In contrast to replaying execution on uniprocessors, deterministic replay on multiprocessors is very challenging to implement efficiently because of the need to reproduce the order or values read by shared memory operations performed by multiple threads. In this paper, we present DoublePlay, a new way to efficiently guarantee replay on commodity multiprocessors. Our key insight is that one can use the simpler and faster mechanisms of single-processor record and replay, yet still achieve the scalability offered by multiple cores, by using an additional execution to parallelize the record and replay of an application. DoublePlay timeslices multiple threads on a single processor, then runs multiple time intervals (epochs) of the program concurrently on separate processors. This strategy, which we call uniparallelism, makes logging much easier because each epoch runs on a single processor (so threads in an epoch never simultaneously access the same memory) and different epochs operate on different copies of the memory. Thus, rather than logging the order of shared-memory accesses, we need only log the order in which threads in an epoch are timesliced on the processor. DoublePlay runs an additional execution of the program on multiple processors to generate checkpoints so that epochs run in parallel. We evaluate DoublePlay on a variety of client, server, and scientific parallel benchmarks; with spare cores, DoublePlay reduces logging overhead to an average of 15\% with two worker threads and 28\% with four threads.},
booktitle = {Proceedings of the Sixteenth International Conference on Architectural Support for Programming Languages and Operating Systems},
pages = {15–26},
numpages = {12},
keywords = {uniparallelism, deterministic replay},
location = {Newport Beach, California, USA},
series = {ASPLOS XVI}
}

@inproceedings {mozilarr,
author = {Robert O{\textquoteright}Callahan and Chris Jones and Nathan Froyd and Kyle Huey and Albert Noll and Nimrod Partush},
title = {Engineering Record and Replay for Deployability},
booktitle = {2017 USENIX Annual Technical Conference (USENIX ATC 17)},
year = {2017},
isbn = {978-1-931971-38-6},
address = {Santa Clara, CA},
pages = {377--389},
url = {https://www.usenix.org/conference/atc17/technical-sessions/presentation/ocallahan},
publisher = {USENIX Association},
month = jul
}

@inproceedings{anduril,
author = {Pan, Jia and Wu, Haoze and Leesatapornwongsa, Tanakorn and Nath, Suman and Huang, Peng},
title = {Efficient Reproduction of Fault-Induced Failures in Distributed Systems with Feedback-Driven Fault Injection},
year = {2024},
isbn = {9798400712517},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3694715.3695979},
doi = {10.1145/3694715.3695979},
abstract = {Debugging a failure usually requires reproducing it first. This can be hard for failures in production distributed systems, where bugs are exposed only by some unusual faulty events. While fault injection testing becomes popular, existing solutions are designed for bug finding. They are ineffective and inefficient to reproduce a specific failure during debugging.We explore a new type of fault injection technique for quickly reproducing a given fault-induced production failure in distributed systems. We present a tool, Anduril, that uses static causal analysis and a novel feedback-driven algorithm to quickly search the enormous fault space for the root-cause fault and timing. We evaluate Anduril on 22 real-world complex fault-induced failures from five large-scale distributed systems. Anduril reproduced all failures by identifying and injecting the root-cause faults at the right time, in a median of 8 minutes.},
booktitle = {Proceedings of the ACM SIGOPS 30th Symposium on Operating Systems Principles},
pages = {46–62},
numpages = {17},
keywords = {failure reproduction, fault injection, distributed systems, debugging},
location = {Austin, TX, USA},
series = {SOSP '24}
}


@misc{ebpfverifier,
	Title = { eBPF verifier.},
	url = {https://docs.ebpf.io/linux/concepts/verifier/},
	note = {Accessed: 2025-01-01}
}

@misc{ebpftracepoints,
	Title = { eBPF tracepoints.},
	url = {https://docs.ebpf.io/linux/program-type/BPF_PROG_TYPE_TRACEPOINT/},
	note = {Accessed: 2025-01-01}
}

@misc{ebpfuprobes,
	Title = { eBPF user function probes.},
	url = {https://docs.kernel.org/trace/uprobetracer.html},
	note = {Accessed: 2025-01-01}
}


@misc{ebpftc,
	Title = { eBPF tc classifier.},
	url = {https://docs.ebpf.io/linux/program-type/BPF_PROG_TYPE_SCHED_CLS/},
	note = {Accessed: 2025-01-01}
}

@misc{kprobe,
	Title = { eBPF kernel probe.},
	url = {https://docs.ebpf.io/linux/program-type/BPF_PROG_TYPE_KPROBE/},
	note = {Accessed: 2025-01-01}
}
@misc{ksyscall,
	Title = { eBPF kernel syscall probe.},
	url = {https://docs.ebpf.io/ebpf-library/libbpf/ebpf/BPF_KSYSCALL/},
	note = {Accessed: 2025-01-01}
}

@misc{redisraft43,
	Title = { RedisRaft bug 43.},
	url = {https://github.com/RedisLabs/redisraft/issues/43/},
	note = {Accessed: 2025-01-01}
}

@misc{procfs,
	Title = { procfs - interface to the proc pseudo-filesystem on linux},
	url = {https://crates.io/crates/procfs},
	note = {Accessed: 2025-01-01}
}

@misc{libbpfrs,
	Title = { libbpf-rs - Idiomatic Rust wrapper around libbpf.},
	url = {https://github.com/libbpf/libbpf-rs},
	note = {Accessed: 2025-01-01}
}

@misc{tendermint,
	Title = { Tendermint - Byzantine Fault Tolerant (BFT) middleware.},
	url = {https://github.com/tendermint/tendermint},
	note = {Accessed: 2025-01-01}
}
@misc{solana,
	Title = { Solana - High-performance blockchain platform.},
	url = {https://github.com/solana-labs/solana},
	note = {Accessed: 2025-01-01}
}
@misc{cometbft,
	Title = { CometBFT -  Byzantine Fault Tolerant (BFT) middleware.},
	url = {https://github.com/cometbft/cometbft},
	note = {Accessed: 2025-01-01}
}

@inproceedings{ycsb,
  author       = {Brian F. Cooper and
                  Adam Silberstein and
                  Erwin Tam and
                  Raghu Ramakrishnan and
                  Russell Sears},
  title        = {Benchmarking cloud serving systems with {YCSB}},
  booktitle    = {SoCC},
  pages        = {143--154},
  publisher    = {{ACM}},
  year         = {2010}
}

@inproceedings{mallory,
author = {Meng, Ruijie and P\^{\i}rlea, George and Roychoudhury, Abhik and Sergey, Ilya},
title = {Greybox Fuzzing of Distributed Systems},
year = {2023},
isbn = {9798400700507},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3576915.3623097},
doi = {10.1145/3576915.3623097},
abstract = {Grey-box fuzzing is the lightweight approach of choice for finding bugs in sequential programs. It provides a balance between efficiency and effectiveness by conducting a biased random search over the domain of program inputs using a feedback function from observed test executions. For distributed system testing, however, the state-of-practice is represented today by only black-box tools that do not attempt to infer and exploit any knowledge of the system's past behaviours to guide the search for bugs.In this work, we present MALLORY: the first framework for grey-box fuzz-testing of distributed systems. Unlike popular black-box distributed system fuzzers, such as JEPSEN, that search for bugs by randomly injecting network partitions and node faults or by following human-defined schedules, MALLORY is adaptive. It exercises a novel metric to learn how to maximize the number of observed system behaviors by choosing different sequences of faults, thus increasing the likelihood of finding new bugs. Our approach relies on timeline-driven testing. MALLORY dynamically constructs Lamport timelines of the system behaviour and further abstracts these timelines into happens-before summaries, which serve as a feedback function guiding the fuzz campaign. Subsequently, MALLORY reactively learns a policy using Q-learning, enabling it to introduce faults guided by its real-time observation of the summaries.We have evaluated MALLORY on a diverse set of widely-used industrial distributed systems. Compared to the start-of-the-art black-box fuzzer JEPSEN, MALLORY explores 54.27\% more distinct states within 24 hours while achieving a speed-up of 2.24X. At the same time, MALLORY finds bugs 1.87X faster, thereby finding more bugs within the given time budget. MALLORY discovered 22 zero-day bugs (of which 18 were confirmed by developers), including 10 new vulnerabilities, in rigorously tested distributed systems such as Braft, Dqlite and Redis. 6 new CVEs have been assigned.},
booktitle = {Proceedings of the 2023 ACM SIGSAC Conference on Computer and Communications Security},
pages = {1615–1629},
numpages = {15},
keywords = {distributed systems, greybox fuzzing, reactive system testing},
location = {Copenhagen, Denmark},
series = {CCS '23}
}

@inproceedings {legolas,
author = {Haoze Wu and Jia Pan and Peng Huang},
title = {Efficient Exposure of Partial Failure Bugs in Distributed Systems with Inferred Abstract States},
booktitle = {21st USENIX Symposium on Networked Systems Design and Implementation (NSDI 24)},
year = {2024},
isbn = {978-1-939133-39-7},
address = {Santa Clara, CA},
pages = {1267--1283},
url = {https://www.usenix.org/conference/nsdi24/presentation/wu-haoze},
publisher = {USENIX Association},
month = apr
}

@inproceedings{reprolite,
author = {Li, Kaituo and Joshi, Pallavi and Gupta, Aarti and Ganai, Malay K.},
title = {ReproLite: A Lightweight Tool to Quickly Reproduce Hard System Bugs},
year = {2014},
isbn = {9781450332521},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2670979.2671004},
doi = {10.1145/2670979.2671004},
abstract = {Cloud systems have become ubiquitous today -- they are used to store and process the tremendous amounts of data being generated by Internet users. These systems run on hundreds of commodity machines, and have a huge amount of non-determinism (thousands of threads and hundreds of processes) in their execution. Therefore, bugs that occur in cloud systems are hard to understand, reproduce, and fix. The state-of-the-art of debugging in the industry is to log messages during execution, and refer to those messages later in case of errors. In ReproLite, we augment the already widespread process of debugging using logs by enabling testers to quickly and easily specify the conjectures that they form regarding the cause of an error (or bug) from execution logs, and to also automatically validate those conjectures.ReproLite includes a Domain Specific Language (DSL) that allows testers to specify all aspects of a potential scenario (e.g., specific workloads, execution operations and their orders, environment non-determinism) that causes a given bug. Given such a scenario, ReproLite can enforce the conditions in the scenario during system execution. Potential buggy scenarios can also be automatically generated from a sequence of log messages that a tester believes indicates the cause of the bug. We have experimented ReproLite with 11 bugs from two popular cloud systems, Cassandra and HBase. We were able to reproduce all of the bugs using ReproLite. We report on our experience with using ReproLite on those bugs.},
booktitle = {Proceedings of the ACM Symposium on Cloud Computing},
pages = {1–13},
numpages = {13},
keywords = {Lightweight, Hard System Bug, Debugging, Cloud Computing},
location = {Seattle, WA, USA},
series = {SOCC '14}
}

@INPROCEEDINGS{toolbft,
  author={Wang, Ping-Lun and Chao, Tzu-Wei and Wu, Chia-Chien and Hsiao, Hsu-Chun},
  booktitle={2022 52nd Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN)},
  title={Tool: An Efficient and Flexible Simulator for Byzantine Fault-Tolerant Protocols},
  year={2022},
  volume={},
  number={},
  pages={287-294},
  keywords={Fault tolerance;Protocols;Fault tolerant systems;Safety;Behavioral sciences;BFT Protocols;Simulation Tool;Byzantine Fault},
  doi={10.1109/DSN53405.2022.00038}}


@article{bftcc,
author = {Dr\u{a}goi, Cezara and Enea, Constantin and Ozkan, Burcu Kulahcioglu and Majumdar, Rupak and Niksic, Filip},
title = {Testing consensus implementations using communication closure},
year = {2020},
issue_date = {November 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {OOPSLA},
url = {https://doi.org/10.1145/3428278},
doi = {10.1145/3428278},
abstract = {Large scale production distributed systems are difficult to design and test. Correctness must be ensured when processes run asynchronously, at arbitrary rates relative to each other, and in the presence of failures, e.g., process crashes or message losses. These conditions create a huge space of executions that is difficult to explore in a principled way. Current testing techniques focus on systematic or randomized exploration of all executions of an implementation while treating the implemented algorithms as black boxes. On the other hand, proofs of correctness of many of the underlying algorithms often exploit semantic properties that reduce reasoning about correctness to a subset of behaviors. For example, the communication-closure property, used in many proofs of distributed consensus algorithms, shows that every asynchronous execution of the algorithm is equivalent to a lossy synchronous execution, thus reducing the burden of proof to only that subset. In a lossy synchronous execution, processes execute in lock-step rounds, and messages are either received in the same round or lost forever—such executions form a small subset of all asynchronous ones. We formulate the communication-closure hypothesis, which states that bugs in implementations of distributed consensus algorithms will already manifest in lossy synchronous executions and present a testing algorithm based on this hypothesis. We prioritize the search space based on a bound on the number of failures in the execution and the rate at which these failures are recovered. We show that a random testing algorithm based on sampling lossy synchronous executions can empirically find a number of bugs—including previously unknown ones—in production distributed systems such as Zookeeper, Cassandra, and Ratis, and also produce more understandable bug traces.},
journal = {Proc. ACM Program. Lang.},
month = nov,
articleno = {210},
numpages = {29},
keywords = {Randomized testing, Distributed consensus, Communication closure}
}

@misc{twins,
      title={Twins: BFT Systems Made Robust},
      author={Shehar Bano and Alberto Sonnino and Andrey Chursin and Dmitri Perelman and Zekun Li and Avery Ching and Dahlia Malkhi},
      year={2022},
      eprint={2004.10617},
      archivePrefix={arXiv},
      primaryClass={cs.CR},
      url={https://arxiv.org/abs/2004.10617},
}

@article{byzzfuzz,
author = {Winter, Levin N. and Buse, Florena and de Graaf, Daan and von Gleissenthall, Klaus and Kulahcioglu Ozkan, Burcu},
title = {Randomized Testing of Byzantine Fault Tolerant Algorithms},
year = {2023},
issue_date = {April 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {OOPSLA1},
url = {https://doi.org/10.1145/3586053},
doi = {10.1145/3586053},
abstract = {Byzantine fault-tolerant algorithms promise agreement on a correct value, even if a subset of processes can deviate from the algorithm arbitrarily. While these algorithms provide strong guarantees in theory, in practice, protocol bugs and implementation mistakes may still cause them to go wrong. This paper introduces ByzzFuzz, a simple yet effective method for automatically finding errors in implementations of Byzantine fault-tolerant algorithms through randomized testing. ByzzFuzz detects fault-tolerance bugs by injecting randomly generated network and process faults into their executions. To navigate the space of possible process faults, ByzzFuzz introduces small-scope message mutations which mutate the contents of the protocol messages by applying small changes to the original message either in value (e.g., by incrementing the round number) or in time (e.g., by repeating a proposal value from a previous message). We find that small-scope mutations, combined with insights from the testing and fuzzing literature, are effective at uncovering protocol logic and implementation bugs in real-world fault-tolerant systems.

We implemented ByzzFuzz and applied it to test the production implementations of two popular blockchain systems, Tendermint and Ripple, and an implementation of the seminal PBFT protocol. ByzzFuzz detected several bugs in the implementation of PBFT, a potential liveness violation in Tendermint, and materialized two theoretically described vulnerabilities in Ripple’s XRP Ledger Consensus Algorithm. Moreover, we discovered a previously unknown fault-tolerance bug in the production implementation of Ripple, which is confirmed by the developers and fixed.},
journal = {Proc. ACM Program. Lang.},
month = apr,
articleno = {101},
numpages = {32},
keywords = {Random testing, Distributed consensus, Byzantine fault-tolerance}
}

@INPROCEEDINGS{tyr,
  author={Chen, Yuanliang and Ma, Fuchen and Zhou, Yuanhang and Jiang, Yu and Chen, Ting and Sun, Jiaguang},
  booktitle={2023 IEEE Symposium on Security and Privacy (SP)},
  title={Tyr: Finding Consensus Failure Bugs in Blockchain System with Behaviour Divergent Model},
  year={2023},
  volume={},
  number={},
  pages={2517-2532},
  keywords={Social networking (online);Computer bugs;Earth Observing System;Detectors;Fabrics;Real-time systems;Blockchains},
  doi={10.1109/SP46215.2023.10179386}}

@article{bft_survey,
author = {Wang, Xin and Duan, Sisi and Clavin, James and Zhang, Haibin},
title = {BFT in Blockchains: From Protocols to Use Cases},
year = {2022},
issue_date = {January 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {54},
number = {10s},
issn = {0360-0300},
url = {https://doi.org/10.1145/3503042},
doi = {10.1145/3503042},
abstract = {A blockchain is a distributed system that achieves strong security guarantees in storing, managing, and processing data. All blockchains achieve a common goal: building a decentralized system that provides a trustworthy service in an untrustworthy environment. A blockchain builds a Byzantine fault-tolerant (BFT) system in which decentralized nodes run a protocol to reach an agreement on the common system state. In this article, we focus on the research of BFT protocols. In particular, we categorize BFT protocols according to both the system models and workflow. We seek to answer these important questions: How has the research in BFT evolved in the past four decades, especially with the rise of blockchains? What are the driven needs for BFT research in the future?},
journal = {ACM Comput. Surv.},
month = sep,
articleno = {209},
numpages = {37},
keywords = {Blockchains, consensus, Byzantine fault tolerance, survey}
}

@INPROCEEDINGS{tla+microsoft,
  author={Hackett, Finn and Rowe, Joshua and Kuppe, Markus Alexander},
  booktitle={2023 IEEE/ACM 45th International Conference on Software Engineering: Software Engineering in Practice (ICSE-SEIP)},
  title={Understanding Inconsistency in Azure Cosmos DB with TLA+},
  year={2023},
  volume={},
  number={},
  pages={1-12},
  keywords={Industries;Semantics;Computer bugs;Focusing;Documentation;Model checking;Behavioral sciences;cloud computing;formal methods;model checking;documentation},
  doi={10.1109/ICSE-SEIP58684.2023.00006}}

@INPROCEEDINGS{logs,
  author={Gholamian, Sina and Ward, Paul A. S.},
  booktitle={2021 40th International Symposium on Reliable Distributed Systems (SRDS)},
  title={What Distributed Systems Say: A Study of Seven Spark Application Logs},
  year={2021},
  volume={},
  number={},
  pages={222-232},
  keywords={Costs;Runtime;Benchmark testing;Software systems;Supercomputers;Sparks;Reliability;logging statement;log verbosity level;log4j;logging cost analysis;information gain;entropy;distributed systems;system failure;Spark},
  doi={10.1109/SRDS53918.2021.00030}}


@ARTICLE{dio,
  author={Esteves, Tânia and Macedo, Ricardo and Oliveira, Rui and Paulo, João},
  journal={IEEE Access},
  title={Toward a Practical and Timely Diagnosis of Application’s I/O Behavior},
  year={2023},
  volume={11},
  number={},
  pages={110184-110207},
  keywords={Pipelines;Behavioral sciences;Data visualization;Information filters;Task analysis;Real-time systems;Storage management;Input-output programs;Storage systems;I/O diagnosis;tracing;analysis},
  doi={10.1109/ACCESS.2023.3322104}}

  @article{avizienis2001fundamental,
  title={Fundamental concepts of dependability},
  author={Avizienis, Algirdas and Laprie, Jean-Claude and Randell, Brian and others},
  journal={Technical Report Series-University of Newcastle upon Tyne Computing Science},
  year={2001},
  publisher={University of Newcastle upon Tyne}
}
